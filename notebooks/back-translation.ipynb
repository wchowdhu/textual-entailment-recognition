{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install google_trans_new\n!pip install googletrans==3.1.0a0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\n# from google_trans_new import google_translator \nimport time\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_sp_path = '../input/train-val-split/train_df_sp.csv'\nvalidation_df_sp_path = '../input/train-val-split/validation_df_sp.csv'\n\nif train_df_sp_path and validation_df_sp_path:\n    # Use the same train-val split to back translate input training data\n    print (\"Loading training and validation split csv files...\")\n    train_df_sp = pd.read_csv(train_df_sp_path)\n    validation_df_sp = pd.read_csv(validation_df_sp_path)\nelse:\n    # Create train-val split data and save as csv file\n    print (\"Creating training and validation split csv files...\")\n    # Stratify ensures that each sub-set contains approximately the same percentage of samples of each target class as the original set.\n    train_df_sp, validation_df_sp = train_test_split(train_df, stratify=train_df.label.values, \n                                                      random_state=42, \n                                                      test_size=0.2, shuffle=True)\n\n\n    train_df_sp.reset_index(drop=True, inplace=True)\n    validation_df_sp.reset_index(drop=True, inplace=True)\n\n    train_df_sp.to_csv('train_df_sp.csv', index=False)\n    validation_df_sp.to_csv('validation_df_sp.csv', index=False)\n    \n    \n# check the number of rows and columns in the subsets after split\nprint(\"Training data shape after split: {}\".format(train_df_sp.shape))\nprint(\"Validation data shape after split: {}\".format(validation_df_sp.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def back_translate(train_df, target_lang='fr'):\n    \n    print (\"Back translating training input for target language: {}\".format(target_lang))\n        \n    train_bt = train_df.copy()\n    df_list = []\n    limit_before_timeout = 100\n    timeout = 5\n    \n    translator = Translator() \n    # Add functions to back translate input sentences\n    def target_translate(x, target_lang):\n        translation = translator.translate(x, dest=target_lang)\n        return translation.text\n    def source_translate(x, source_lang):\n        translation = translator.translate(x, dest=source_lang) \n        return translation.text \n    \n    for i in tqdm(range(len(train_bt))):\n        entry = train_bt.loc[[i]]\n        source_lang = entry.lang_abv.values.tolist()[0]\n        if source_lang == 'zh':\n            #print(googletrans.LANGUAGES) \n            source_lang = 'zh-cn' #'zh' not in googletrans.LANGUAGES\n        \n        if (i!=0) and (i%limit_before_timeout == 0): #apply timeout after every 100 iterations \n            print('Iteration {} of {}'.format(i, len(train_bt)))\n            time.sleep(timeout)\n        \n        # Back translate premise sentence\n        entry['premise'] = entry['premise'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['premise'] = entry['premise'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)\n        \n        # Back translate hypothesis sentence\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)\n    \n        df_list.append(entry)\n    \n    train_bt = pd.concat(df_list, ignore_index=True)\n    return train_bt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlang_abvs = train_df_sp['lang_abv'].unique().tolist()\nlang_abvs = ['zh-cn' if lang == 'zh' else lang for lang in lang_abvs]\nfor lang in lang_abvs:\n    #sample input training data to back translate\n    train_sampled = (train_df_sp.groupby('language', group_keys=False).apply(lambda x: x.sample(min(len(x), 1000)))).sample(frac = 1).reset_index(drop=True)\n    back_trans_df = back_translate(train_sampled, target_lang=lang)\n    print(\"Shape of input data with back translation for {}: {}\".format(lang, back_trans_df.shape))\n    back_trans_df.to_csv('back_translations_{}.csv'.format(lang), index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}