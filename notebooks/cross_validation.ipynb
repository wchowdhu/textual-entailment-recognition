{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade allennlp\n!pip install transformers==4.3.0","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.0.1)\nCollecting pip\n  Downloading pip-21.1-py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 2.9 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 21.0.1\n    Uninstalling pip-21.0.1:\n      Successfully uninstalled pip-21.0.1\nSuccessfully installed pip-21.1\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nRequirement already satisfied: allennlp in /opt/conda/lib/python3.7/site-packages (2.3.0)\nCollecting allennlp\n  Downloading allennlp-2.4.0-py3-none-any.whl (625 kB)\n\u001b[K     |████████████████████████████████| 625 kB 2.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from allennlp) (8.7.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from allennlp) (2.10.0)\nRequirement already satisfied: filelock<3.1,>=3.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (3.0.12)\nRequirement already satisfied: wandb<0.11.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (0.10.26)\nRequirement already satisfied: torchvision<0.10.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from allennlp) (0.8.1)\nRequirement already satisfied: jsonnet>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (0.17.0)\nCollecting huggingface-hub>=0.0.8\n  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\nRequirement already satisfied: overrides==3.1.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (3.1.0)\nRequirement already satisfied: spacy<3.1,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (2.3.5)\nRequirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.7/site-packages (from allennlp) (4.59.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from allennlp) (0.24.1)\nRequirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.7/site-packages (from allennlp) (2.25.1)\nRequirement already satisfied: lmdb in /opt/conda/lib/python3.7/site-packages (from allennlp) (1.2.0)\nRequirement already satisfied: boto3<2.0,>=1.14 in /opt/conda/lib/python3.7/site-packages (from allennlp) (1.17.53)\nRequirement already satisfied: transformers<4.6,>=4.1 in /opt/conda/lib/python3.7/site-packages (from allennlp) (4.5.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from allennlp) (3.2.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from allennlp) (1.5.4)\nRequirement already satisfied: torch<1.9.0,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from allennlp) (1.7.0)\nRequirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.7/site-packages (from allennlp) (2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from allennlp) (1.19.5)\nRequirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (from allennlp) (6.2.3)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from allennlp) (0.1.95)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.14->allennlp) (0.3.7)\nRequirement already satisfied: botocore<1.21.0,>=1.20.53 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.14->allennlp) (1.20.53)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.53->boto3<2.0,>=1.14->allennlp) (2.8.1)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.53->boto3<2.0,>=1.14->allennlp) (1.26.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.8->allennlp) (3.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.53->boto3<2.0,>=1.14->allennlp) (1.15.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18->allennlp) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18->allennlp) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18->allennlp) (2.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (0.7.4)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.5)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (49.6.0.post20210108)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.0.8->allennlp) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.0.8->allennlp) (3.7.4.3)\nRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX>=1.2->allennlp) (3.15.8)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<1.9.0,>=1.6.0->allennlp) (0.18.2)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<1.9.0,>=1.6.0->allennlp) (0.6)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.2.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers<4.6,>=4.1->allennlp) (0.10.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<4.6,>=4.1->allennlp) (2021.3.17)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers<4.6,>=4.1->allennlp) (0.0.45)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers<4.6,>=4.1->allennlp) (20.9)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.1.2)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.1.14)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.5.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.3.1)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.8.0)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.0.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.4.0)\nRequirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.0)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.7)\nRequirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (3.0.5)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers<4.6,>=4.1->allennlp) (2.4.7)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest->allennlp) (1.10.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from pytest->allennlp) (0.10.2)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest->allennlp) (1.1.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->allennlp) (20.3.0)\nRequirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->allennlp) (0.13.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers<4.6,>=4.1->allennlp) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->allennlp) (2.1.0)\nInstalling collected packages: huggingface-hub, allennlp\n  Attempting uninstall: allennlp\n    Found existing installation: allennlp 2.3.0\n    Uninstalling allennlp-2.3.0:\n      Successfully uninstalled allennlp-2.3.0\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nSuccessfully installed allennlp-2.4.0 huggingface-hub-0.0.8\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nCollecting transformers==4.3.0\n  Downloading transformers-4.3.0-py3-none-any.whl (1.8 MB)\n\u001b[K     |████████████████████████████████| 1.8 MB 2.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (1.19.5)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (0.0.45)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (4.59.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (2021.3.17)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (20.9)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (3.4.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (0.10.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (3.0.12)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.3.0) (2.25.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.3.0) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.3.0) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.3.0) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.0) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.0) (1.26.4)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.0) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.3.0) (2020.12.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.0) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.0) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.3.0) (1.0.1)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.5.1\n    Uninstalling transformers-4.5.1:\n      Successfully uninstalled transformers-4.5.1\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nSuccessfully installed transformers-4.3.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets==1.2.1 #to load xnli and mnli datasets from huggingface library\n!pip install googletrans==3.1.0a0","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nCollecting datasets==1.2.1\n  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n\u001b[K     |████████████████████████████████| 159 kB 2.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (0.70.11.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (0.3.3)\nCollecting tqdm<4.50.0,>=4.27\n  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 4.0 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (3.0.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (3.4.0)\nCollecting xxhash\n  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n\u001b[K     |████████████████████████████████| 243 kB 13.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (1.2.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (1.19.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.2.1) (2.25.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.2.1) (1.26.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.2.1) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.2.1) (3.7.4.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.2.1) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.2.1) (2021.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\nInstalling collected packages: xxhash, tqdm, datasets\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.59.0\n    Uninstalling tqdm-4.59.0:\n      Successfully uninstalled tqdm-4.59.0\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nSuccessfully installed datasets-1.2.1 tqdm-4.49.0 xxhash-2.0.2\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\nCollecting googletrans==3.1.0a0\n  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\nCollecting httpx==0.13.3\n  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 1.1 MB/s eta 0:00:01\n\u001b[?25hCollecting hstspreload\n  Downloading hstspreload-2020.12.22-py3-none-any.whl (994 kB)\n\u001b[K     |████████████████████████████████| 994 kB 4.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\nCollecting chardet==3.*\n  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[K     |████████████████████████████████| 133 kB 12.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\nCollecting httpcore==0.9.*\n  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 633 kB/s  eta 0:00:01\n\u001b[?25hCollecting sniffio\n  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\nCollecting rfc3986<2,>=1.3\n  Downloading rfc3986-1.4.0-py2.py3-none-any.whl (31 kB)\nCollecting h2==3.*\n  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 2.2 MB/s  eta 0:00:01\n\u001b[?25hCollecting h11<0.10,>=0.8\n  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting hpack<4,>=3.0\n  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nCollecting hyperframe<6,>=5.2.0\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=271ca5f26be47d0ded7fb030da87d3515a9c42bff09dbddbfbbf392eb5ab4b46\n  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\nSuccessfully built googletrans\nInstalling collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, chardet, httpx, googletrans\n  Attempting uninstall: chardet\n    Found existing installation: chardet 4.0.0\n    Uninstalling chardet-4.0.0:\n      Successfully uninstalled chardet-4.0.0\n\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\ndistutils: /opt/conda/include/python3.7m/UNKNOWN\nsysconfig: /opt/conda/include/python3.7m\u001b[0m\n\u001b[33mWARNING: Additional context:\nuser = False\nhome = None\nroot = None\nprefix = None\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.1.0 which is incompatible.\naiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.20.53 which is incompatible.\u001b[0m\nSuccessfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport os.path\nfrom os import path\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.034409,"end_time":"2021-02-09T22:37:48.051173","exception":false,"start_time":"2021-02-09T22:37:48.016764","status":"completed"},"tags":[],"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/contradictory-my-dear-watson/sample_submission.csv\n/kaggle/input/contradictory-my-dear-watson/train.csv\n/kaggle/input/contradictory-my-dear-watson/test.csv\n/kaggle/input/backtranslations-for-data-augmentation-in-nlp/back_translated_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"papermill":{"duration":0.028853,"end_time":"2021-02-09T22:37:48.100737","exception":false,"start_time":"2021-02-09T22:37:48.071884","status":"completed"},"tags":[],"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig, TFAutoModel    \nfrom transformers import (XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel)            \n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D, LayerNormalization\nfrom keras.layers.normalization import BatchNormalization\n\nfrom datasets import load_dataset, list_datasets\nfrom tqdm import tqdm\n\nfrom googletrans import Translator\n# from google_trans_new import google_translator \n\nimport time\nimport glob\n\n# Resets all state generated by Keras\nK.clear_session()\n\n# For reproducibility\nnp.random.seed(0)","metadata":{"papermill":{"duration":11.59848,"end_time":"2021-02-09T22:37:59.720729","exception":false,"start_time":"2021-02-09T22:37:48.122249","status":"completed"},"tags":[],"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Found TPU: ', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"papermill":{"duration":5.453391,"end_time":"2021-02-09T22:38:05.194582","exception":false,"start_time":"2021-02-09T22:37:59.741191","status":"completed"},"tags":[],"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found TPU:  grpc://10.0.0.2:8470\nNumber of replicas: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configuration Settings\nEPOCHS = 5\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 120\nPATIENCE = 1\nLEARNING_RATE = 1e-5\nAUTO = tf.data.experimental.AUTOTUNE\nLOAD_XNLI = True\nLOAD_MNLI = True\nBACK_TRANSLATE = True\nBT_DIR = '../input/backtranslations-for-data-augmentation-in-nlp' #directory containing the back translations of input training data","metadata":{"papermill":{"duration":0.030523,"end_time":"2021-02-09T22:38:05.549919","exception":false,"start_time":"2021-02-09T22:38:05.519396","status":"completed"},"tags":[],"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\n# check the number of rows and columns in the datasets\nprint(\"Training data shape: {}\".format(train_df.shape))\nprint(\"Test data shape: {}\\n\".format(test_df.shape))\n\ntrain_df.head()","metadata":{"papermill":{"duration":0.228832,"end_time":"2021-02-09T22:38:05.446368","exception":false,"start_time":"2021-02-09T22:38:05.217536","status":"completed"},"tags":[],"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Training data shape: (12120, 6)\nTest data shape: (5195, 5)\n\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"           id                                            premise  \\\n0  5130fd2cb5  and these comments were considered in formulat...   \n1  5b72532a0b  These are issues that we wrestle with in pract...   \n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n3  5622f0c60b  you know they can't really defend themselves l...   \n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis lang_abv language  label  \n0  The rules developed in the interim were put to...       en  English      0  \n1  Practice groups are not permitted to work on t...       en  English      2  \n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n3  They can't defend themselves because of their ...       en  English      0  \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5130fd2cb5</td>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5b72532a0b</td>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3931fbe82a</td>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>fr</td>\n      <td>French</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5622f0c60b</td>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86aaa48b45</td>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>th</td>\n      <td>Thai</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def process_xnli_data(test_df, all_keys=False, only_train=False): \n    if only_train:\n        print (\"Splitting by train data only\")\n        split = 'train'\n    elif all_keys:\n        print (\"Splitting by all keys\")\n        split = 'validation+test+train[:5%]'\n    else:\n        print (\"Splitting by validation and test data\")\n        split = 'validation+test'\n    \n    print(\"Loading XNLI data...\")\n    print(\"Split: \", split)\n    \n    dataset = load_dataset('xnli', 'all_languages', split=split) #returns a Dataset object  \n    print(dataset)\n    \n    entries = []   \n    for entry in tqdm(dataset): \n        hypothesis_langs = entry['hypothesis']['language'] #list of 15 lang string values\n        hypothesis_values = entry['hypothesis']['translation'] #list of 15 hypothesis string values\n\n        premise_langs = list(entry['premise'].keys()) #list of 15 lang string values\n        premise_values = list(entry['premise'].values()) #list of 15 premise string values\n\n        labels = [entry['label']]*len(hypothesis_langs) #all 15 languages for the same example have same label \n\n        if premise_langs == hypothesis_langs: #the languages in premise and hypothesis are in same order\n#             values = list(zip(premise_values, hypothesis_values, hypothesis_langs, labels))\n            values = list(zip(premise_values, hypothesis_values, labels))\n            entries += values\n\n#     xnli_df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'lang_abv', 'label']) #create dataframe for each key\n    xnli_df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'label']) #create dataframe for each key\n    \n    # Get the number of missing data points per column\n    missing_values_xnli = xnli_df.isnull().sum() \n\n    print(\"Number of missing data points per column in XNLI corpus:\")\n    print (missing_values_xnli)\n\n    # Drop the missing value rows\n    xnli_df.dropna(axis=0, inplace=True)\n    print(\"Total number of data examples in XNLI corpus after dropping NA values: {}\".format(xnli_df.shape[0]))\n    \n    ################## Delete duplicate rows between test_df and xnli_df in xnli corpus #########################\n    xnli_df = xnli_df.drop_duplicates() #drop duplicate rows\n    print('Total number of data examples in XNLI corpus after dropping duplicate values: {}'.format(xnli_df.shape[0]))\n    \n    print('Test data shape: {}'.format(test_df.shape))\n    test_df = test_df.drop_duplicates() #drop duplicate rows\n    print('Test data shape after dropping duplicate rows: {}'.format(test_df.shape))\n\n    df_merge = pd.merge(xnli_df, test_df, on=['premise','hypothesis'], how='inner')\n    xnli_df = xnli_df.append(df_merge, ignore_index=True) \n\n    xnli_df['duplicated'] = xnli_df.duplicated(subset=['premise','hypothesis'], keep=False) # keep=False marks the duplicated row with a True\n    xnli_df = xnli_df[~xnli_df['duplicated']] # selects only rows which are not duplicated.\n    \n    xnli_df.drop(['duplicated', 'id', 'lang_abv', 'language'], axis = 1, inplace=True) # remove following columns\n    ################## Delete duplicate rows between test_df and xnli_df in xnli corpus #########################\n    \n    print(\"XNLI corpus shape: {}\".format(xnli_df.shape))\n    \n    del dataset #free up space\n    \n    return xnli_df","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def process_mnli_data(validation=False):\n    if validation:\n        keys = ['train', 'validation_matched', 'validation_mismatched']\n    else:\n        keys = ['train']\n    \n    dataset = load_dataset('glue', name='mnli') #returns a DatasetDict object\n    print(dataset)\n    \n    df_dict = {}\n    \n    for key in tqdm(keys):\n        print(\"Processing MNLI {} data...\".format(key))\n        \n        hypothesis = dataset[key]['hypothesis']\n        premise = dataset[key]['premise']\n        labels = dataset[key]['label']\n        \n        entries = list(zip(premise, hypothesis, labels))\n        \n        df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'label']) #create dataframe for each key\n        df_dict[key] = df\n        \n    if validation:\n        mnli_df = pd.concat([df_dict['train'], df_dict['validation_matched'], df_dict['validation_mismatched']], ignore_index=True)\n    else:\n        mnli_df = df_dict['train']\n    \n    # Get the number of missing data points per column\n    missing_values_mnli = mnli_df.isnull().sum() \n\n    print(\"Number of missing data points per column in MNLI corpus:\")\n    print (missing_values_mnli)\n\n    # Drop the missing value rows from training set\n    mnli_df.dropna(axis=0, inplace=True)\n    print(\"Total number of data examples in MNLI corpus after dropping NA values: {}\\n\".format(mnli_df.shape[0]))\n    \n    del dataset #free up space\n    \n    return mnli_df","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def back_translate(train_df, target_lang='fr', sample=True, num_samples_per_lang=1000):\n    if sample: #sample input training data to back translate\n        train_df = train_df.groupby('language', group_keys=False).apply(lambda x: x.sample(min(len(x), num_samples_per_lang))).reset_index(drop=True)  \n\n    df_list = []\n    limit_before_timeout = 100\n    timeout = 5\n    \n    translator = Translator() \n    \n    # Add functions to back translate input sentences\n    def target_translate(x, target_lang):\n        translation = translator.translate(x, dest=target_lang)\n        return translation.text\n    def source_translate(x, source_lang):\n        translation = translator.translate(x, dest=source_lang) \n        return translation.text \n    \n    for i in tqdm(range(len(train_df))):\n        entry = train_df.loc[[i]]\n        source_lang = entry.lang_abv.values.tolist()[0]\n        if source_lang == 'zh':\n            #print(googletrans.LANGUAGES) \n            source_lang = 'zh-cn' #'zh' not in googletrans.LANGUAGES        \n        if (i!=0) and (i%limit_before_timeout == 0): #apply timeout after every 100 iterations \n            print('Iteration {} of {}'.format(i, len(train_df)))\n            time.sleep(timeout)      \n        # Back translate premise sentence\n        entry['premise'] = entry['premise'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['premise'] = entry['premise'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)       \n        # Back translate hypothesis sentence\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)\n        df_list.append(entry)\n    \n    train_bt = pd.concat(df_list, ignore_index=True)\n    print(\"Shape of back-translated training data: {}\".format(train_bt.shape))\n    return train_bt","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def augment_data(df, use_xnli=True, use_mnli=True, use_bt=True, bt_dir=''):\n    df_list = []  \n    if use_bt:\n        if path.isdir(bt_dir):\n            files = glob.glob(bt_dir+'/*.csv')\n            bt_list = []\n            for filename in files:\n                bt_list.append(pd.read_csv(filename))\n            bt_df = pd.concat(bt_list, ignore_index=True)\n#             bt_df.isnull().sum() # we get the number of missing values\n            bt_df.dropna(inplace=True) #remove missing values\n            bt_df.drop_duplicates(inplace=True) #drop duplicate rows  \n            bt_df = bt_df.sample(frac=0.4) #randomly select n examples as back-translated data \n            print(\"Shape of back-translated training data: {}\".format(bt_df.shape))\n            bt_df.to_csv('back_translation_all.csv', index=False)\n        else:\n            bt_df = back_translate(df)\n        bt_df = bt_df.drop(['id', 'lang_abv', 'language'], axis=1)\n        df_list.append(bt_df)\n        del bt_df #free up space \n    if use_xnli:\n        xnli_df = process_xnli_data(test_df)\n#         xnli_train_df = process_xnli_data(test_df, only_train=True) #train data, around 5 million examples\n#         xnli_train_df = xnli_train_df.sample(frac=0.05) #sample 5% of train data\n#         xnli_df = process_xnli_data(test_df) #val+test\n#         xnli_df = xnli_df.append(xnli_train_df, ignore_index=True) #val+test+train[5%]\n#         print ('Final XNLI shape: {}'.format(xnli_df.shape))\n        df_list.append(xnli_df)\n        del xnli_df #free up space \n    if use_mnli:\n        mnli_df = process_mnli_data(validation=False)\n        df_list.append(mnli_df)\n        del mnli_df #free up space\n        \n    train_df = df.drop(['id', 'lang_abv', 'language'], axis=1)\n    \n    if len(df_list) > 0: #augment data\n        augmented_df = pd.concat(df_list, ignore_index=True)\n        train_df = train_df.append(augmented_df, ignore_index=True) \n        \n    train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle data\n    print(\"Augmented data shape before removing duplicate rows: {}\".format(train_df.shape))\n    train_df.drop_duplicates(inplace=True) #drop duplicate rows\n    print(\"Augmented data shape after removing duplicate rows: {}\".format(train_df.shape))\n    return train_df\n\n\ntrain_df = augment_data(train_df, LOAD_XNLI, LOAD_MNLI, BACK_TRANSLATE, BT_DIR) \ntrain_df.head(100)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Shape of back-translated training data: (4778, 6)\nSplitting by validation and test data\nLoading XNLI data...\nSplit:  validation+test\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2697.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016f261ef7974b7eaff8bb2ad798eec0"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2184.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61b4e8822f746788b57da0d19c6ea15"}},"metadata":{}},{"name":"stdout","text":"\nDownloading and preparing dataset xnli/all_languages (download: 461.54 MiB, generated: 1.50 GiB, post-processed: Unknown size, total: 1.95 GiB) to /root/.cache/huggingface/datasets/xnli/all_languages/1.1.0/51ba3a1091acf33fd7c2a54bcbeeee1b1df3ecb127fdca003d31968fa3a1e6a8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466098360.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebbcc12d407d4355b416941f42c14e7b"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=17865352.0, style=ProgressStyle(descrip…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c9b7d1c62ee4c30b91164dde62a4f20"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"  7%|▋         | 553/7500 [00:00<00:01, 5524.42it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset xnli downloaded and prepared to /root/.cache/huggingface/datasets/xnli/all_languages/1.1.0/51ba3a1091acf33fd7c2a54bcbeeee1b1df3ecb127fdca003d31968fa3a1e6a8. Subsequent calls will reuse this data.\nDataset({\n    features: ['premise', 'hypothesis', 'label'],\n    num_rows: 7500\n})\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 7500/7500 [00:01<00:00, 5559.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Number of missing data points per column in XNLI corpus:\npremise       0\nhypothesis    0\nlabel         0\ndtype: int64\nTotal number of data examples in XNLI corpus after dropping NA values: 112500\nTotal number of data examples in XNLI corpus after dropping duplicate values: 112500\nTest data shape: (5195, 5)\nTest data shape after dropping duplicate rows: (5195, 5)\nXNLI corpus shape: (110226, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7826.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721fe2f634a24b2aa6efc8c7e97ffd08"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4473.0, style=ProgressStyle(description…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eba9e0dbdd240e392588d391c9971f5"}},"metadata":{}},{"name":"stdout","text":"\nDownloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=312783507.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8e17d3739e44a3caf7c2a9ff4971ec1"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\nDatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9832\n    })\n    test_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9796\n    })\n    test_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9847\n    })\n})\nProcessing MNLI train data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Number of missing data points per column in MNLI corpus:\npremise       0\nhypothesis    0\nlabel         0\ndtype: int64\nTotal number of data examples in MNLI corpus after dropping NA values: 392702\n\nAugmented data shape before removing duplicate rows: (519826, 3)\nAugmented data shape after removing duplicate rows: (512850, 3)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                              premise  \\\n0                It has an interesting story to tell.   \n1   it's gotten so though that there are so many y...   \n2                          我们也强调与历史，文学和社会研究主题直接相关的剧本。   \n3   This parallel obsession elicited the interesti...   \n4   It seems to me an unnecessary expense to spend...   \n..                                                ...   \n95  Ha! Wolverstone trút sự xuất tinh của việc sin...   \n96  دھماکے میں چھ افراد جاں بحق، تقریبا 1000 زخمی ...   \n97  ابوبکر کے ساتھ ابرام کے ساتھ، بندرگاہ میں، اس ...   \n98  When Hideyoshi built his main castle in the ce...   \n99  yeah um-hum yeah it Jimi Hendrix was the origi...   \n\n                                           hypothesis  label  \n0   It was interesting to explain what happened wh...      1  \n1   There's a very large amount now - it's become ...      0  \n2                                 我们尽量不制作着眼于历史或文学的戏剧。      2  \n3   The obsession elicited a suggestion that all o...      1  \n4   I firmly feel that it is right to spend millio...      2  \n..                                                ...    ...  \n95                            Wolverstone không cười.      2  \n96       دھماکے میں تقریبا ایک ہزار افراد زخمی ہوگئے.      0  \n97  Arabella ایک چھوٹے سے شہر میں آیا جس کی زندگی ...      2  \n98  Hideyoshi building his castle in Osaka in 1583...      2  \n99  Jimmy Hendrix was not the writer of the origin...      2  \n\n[100 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It has an interesting story to tell.</td>\n      <td>It was interesting to explain what happened wh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>it's gotten so though that there are so many y...</td>\n      <td>There's a very large amount now - it's become ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>我们也强调与历史，文学和社会研究主题直接相关的剧本。</td>\n      <td>我们尽量不制作着眼于历史或文学的戏剧。</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This parallel obsession elicited the interesti...</td>\n      <td>The obsession elicited a suggestion that all o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>It seems to me an unnecessary expense to spend...</td>\n      <td>I firmly feel that it is right to spend millio...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Ha! Wolverstone trút sự xuất tinh của việc sin...</td>\n      <td>Wolverstone không cười.</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>دھماکے میں چھ افراد جاں بحق، تقریبا 1000 زخمی ...</td>\n      <td>دھماکے میں تقریبا ایک ہزار افراد زخمی ہوگئے.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>ابوبکر کے ساتھ ابرام کے ساتھ، بندرگاہ میں، اس ...</td>\n      <td>Arabella ایک چھوٹے سے شہر میں آیا جس کی زندگی ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>When Hideyoshi built his main castle in the ce...</td>\n      <td>Hideyoshi building his castle in Osaka in 1583...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>yeah um-hum yeah it Jimi Hendrix was the origi...</td>\n      <td>Jimmy Hendrix was not the writer of the origin...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# check the number of rows and columns in the augmented train data\nprint(\"Augmented train data shape: {}\".format(train_df.shape))","metadata":{"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Augmented train data shape: (512850, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# check distribution of target classes in the augmented data\ncounts = train_df['label'].value_counts()\n\nclass_labels = ['Entailment', 'Neutral', 'Contradiction']\n\ncounts_per_class = [counts[0], counts[1], counts[2]]\n\nplt.figure(figsize = (10,10))\nplt.pie(counts_per_class, labels = class_labels, autopct = '%1.1f%%')\nplt.title(\"Distribution of target classes in the augmented data\")\nplt.show()","metadata":{"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"PRETRAINED_MODEL_TYPES = {\n    'xlmroberta': (XLMRobertaConfig, TFXLMRobertaModel, XLMRobertaTokenizer, 'jplu/tf-xlm-roberta-large')\n}\n\nconfig_class, model_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']","metadata":{"papermill":{"duration":0.028911,"end_time":"2021-02-09T22:38:05.496872","exception":false,"start_time":"2021-02-09T22:38:05.467961","status":"completed"},"tags":[],"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Download vocabulary from huggingface.co and cache.\n# tokenizer = tokenizer_class.from_pretrained(model_name) \ntokenizer = AutoTokenizer.from_pretrained(model_name) #fast tokenizer\n\ntokenizer","metadata":{"papermill":{"duration":3.69166,"end_time":"2021-02-09T22:38:09.263167","exception":false,"start_time":"2021-02-09T22:38:05.571507","status":"completed"},"tags":[],"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a6ba5215924db19d6c12fcc8a3d735"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9cb1a9a3a64bb9a7ff050ce302ede5"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='jplu/tf-xlm-roberta-large', vocab_size=250002, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"},"metadata":{}}]},{"cell_type":"code","source":"def encode(df, tokenizer, max_len=50, cross_val=False):\n    \n    pairs = df[['premise','hypothesis']].values.tolist() #shape=[num_examples]\n    \n    print (\"Encoding...\")\n    encoded_dict = tokenizer.batch_encode_plus(pairs, max_length=max_len, padding=True, truncation=True, \n                                               add_special_tokens=True, return_attention_mask=True)\n    print (\"Complete\")\n    \n    if cross_val:\n        input_word_ids = np.array(encoded_dict['input_ids']) #shape=[num_examples, max_len])\n        input_mask = np.array(encoded_dict['attention_mask']) #shape=[num_examples, max_len]\n    else:\n        input_word_ids = tf.convert_to_tensor(encoded_dict['input_ids'], dtype=tf.int32) #shape=[num_examples, max_len])\n        input_mask = tf.convert_to_tensor(encoded_dict['attention_mask'], dtype=tf.int32) #shape=[num_examples, max_len]\n\n\n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask}    \n    \n    return inputs","metadata":{"papermill":{"duration":0.036046,"end_time":"2021-02-09T22:38:09.437981","exception":false,"start_time":"2021-02-09T22:38:09.401935","status":"completed"},"tags":[],"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def build_model(model_name, max_len=50):\n    \n    tf.random.set_seed(1234)\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    \n    # The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n    base_model = model_class.from_pretrained(model_name)\n#     base_model = TFAutoModel.from_pretrained(model_name)\n    \n#     # Extract pretrained embedding vectors\n#     embedding = base_model([input_word_ids, input_mask])[0] # shape=(batch_size, max_len, embed_size)\n#     # We pass the embedding vectors of only the 'cls' token (at index=0) to the dense layer\n#     sequence_output = embedding[:,0,:] #shape=(batch_size, embed_size)\n\n    \n    ### pooler_output contains the hidden representation of just the ‘[CLS]’ token after additionally being passed to a fully connected layer with tanh activation function.\n    ### output = base_model([input_word_ids, input_mask], training=False) # output from xlmroberta model\n    output = base_model([input_word_ids, input_mask]) # output from xlmroberta model\n    sequence_output = output.pooler_output #shape: [batch_size, embed_size]\n    ### sequence_output = Dropout(0.3)(sequence_output) #add dropout\n    \n#     # Add an LSTM layer to get sentence embeddings\n#     sequence_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, dropout=0.3, recurrent_dropout=0.3))(embedding) # shape=(batch_size, output_size) \n# #     sequence_output = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(150, dropout=0.3, recurrent_dropout=0.3))(embedding) # shape=(batch_size, output_size) \n    \n#     # Add a fully-connected layer\n#     sequence_output = tf.keras.layers.Dense(300, activation=\"relu\")(sequence_output)\n#     sequence_output = BatchNormalization()(sequence_output)\n#     sequence_output = tf.keras.layers.Dropout(0.3)(sequence_output) \n    \n    # Add a classification layer\n    output = Dense(units=3, activation=\"softmax\")(sequence_output)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"papermill":{"duration":0.036703,"end_time":"2021-02-09T22:38:09.499066","exception":false,"start_time":"2021-02-09T22:38:09.462363","status":"completed"},"tags":[],"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_dataset(features, labels, batch_size=BATCH_SIZE, validation=False):\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(len(features))\n    if validation:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    return dataset","metadata":{"papermill":{"duration":0.034168,"end_time":"2021-02-09T22:38:09.557769","exception":false,"start_time":"2021-02-09T22:38:09.523601","status":"completed"},"tags":[],"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_input = encode(train_df, tokenizer=tokenizer, max_len=MAX_LEN, cross_val=True)","metadata":{"papermill":{"duration":2.402471,"end_time":"2021-02-09T22:38:11.985087","exception":false,"start_time":"2021-02-09T22:38:09.582616","status":"completed"},"tags":[],"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Encoding...\nComplete\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ids = train_input['input_word_ids'] #[9696, max_len]\ntrain_mask = train_input['input_mask'] #[9696, max_len]","metadata":{"papermill":{"duration":0.034371,"end_time":"2021-02-09T22:38:12.046227","exception":false,"start_time":"2021-02-09T22:38:12.011856","status":"completed"},"tags":[],"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(train_ids.shape)\nprint(train_mask.shape)","metadata":{"papermill":{"duration":0.036203,"end_time":"2021-02-09T22:38:12.108179","exception":false,"start_time":"2021-02-09T22:38:12.071976","status":"completed"},"tags":[],"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"(512850, 120)\n(512850, 120)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_labels = train_df.label.values","metadata":{"papermill":{"duration":0.035668,"end_time":"2021-02-09T22:38:12.236418","exception":false,"start_time":"2021-02-09T22:38:12.20075","status":"completed"},"tags":[],"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(train_labels.shape)","metadata":{"papermill":{"duration":0.049031,"end_time":"2021-02-09T22:38:12.320062","exception":false,"start_time":"2021-02-09T22:38:12.271031","status":"completed"},"tags":[],"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"(512850,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# prepare cross validation\nfolds = 4\nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n\n# for plotting\nval_loss_list = []\nval_acc_list = []\ntrain_hist_list = []\n\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE)]\n\n# enumerate splits\nfor k, (train_idx, val_idx) in enumerate(kfold.split(train_ids, train_labels)):\n    print ('Fold {} of {}'.format(k+1, folds))\n    print('Train data shape: {}, Validation data shape: {}'.format(len(train_idx), len(val_idx)))\n    print('Train indices: {}, Validation indices: {}'.format(train_idx, val_idx))\n    \n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    training_data = create_dataset((train_ids[train_idx], train_mask[train_idx]), train_labels[train_idx], batch_size=BATCH_SIZE, validation=False)\n    validation_data = create_dataset((train_ids[val_idx], train_mask[val_idx]), train_labels[val_idx], batch_size=BATCH_SIZE, validation=True)\n    \n    # instantiating the model in the strategy scope creates the model on the TPU\n    with strategy.scope():\n        model = build_model(model_name, MAX_LEN)\n        model.summary()\n        \n    n_steps = int(len(train_idx)/BATCH_SIZE)\n    \n    train_history = model.fit(x=training_data, validation_data=validation_data, epochs=EPOCHS, verbose=1, steps_per_epoch=n_steps, callbacks=callbacks)\n    \n    avg_val_loss = np.mean(train_history.history['val_loss'])\n    avg_val_acc = np.mean(train_history.history['val_accuracy'])\n    \n    print ('Average Validation Loss: {}'.format(avg_val_loss))\n    print ('Average Validation Accuracy: {}'.format(avg_val_acc))\n    print('#############################################\\n')\n    \n    val_loss_list.append(avg_val_loss)\n    val_acc_list.append(avg_val_acc)\n    train_hist_list.append(train_history)\n    \n    del model #free up space\n            \n    # Resets all state generated by Keras before training a new model in the next fold\n    K.clear_session()","metadata":{"papermill":{"duration":1674.851104,"end_time":"2021-02-09T23:06:07.202444","exception":false,"start_time":"2021-02-09T22:38:12.35134","status":"completed"},"tags":[],"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Fold 1 of 4\nTrain data shape: 384637, Validation data shape: 128213\nTrain indices: [     0      1      2 ... 512845 512846 512847], Validation indices: [     5      6      9 ... 512844 512848 512849]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7ececd15b04e8bbd4947296ea14c4f"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ntfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            3075        tfxlm_roberta_model[0][1]        \n==================================================================================================\nTotal params: 559,893,507\nTrainable params: 559,893,507\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n  num_elements)\n","output_type":"stream"},{"name":"stdout","text":"3004/3004 [==============================] - 917s 265ms/step - loss: 0.5701 - accuracy: 0.7431 - val_loss: 0.3301 - val_accuracy: 0.8783\nEpoch 2/5\n3004/3004 [==============================] - 778s 259ms/step - loss: 0.3098 - accuracy: 0.8832 - val_loss: 0.3106 - val_accuracy: 0.8882\nEpoch 3/5\n3004/3004 [==============================] - 778s 259ms/step - loss: 0.2361 - accuracy: 0.9139 - val_loss: 0.3166 - val_accuracy: 0.8911\nEpoch 00003: early stopping\nAverage Validation Loss: 0.31910763184229535\nAverage Validation Accuracy: 0.8858851989110311\n#############################################\n\nFold 2 of 4\nTrain data shape: 384637, Validation data shape: 128213\nTrain indices: [     1      2      5 ... 512847 512848 512849], Validation indices: [     0      3      4 ... 512838 512842 512846]\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ntfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            3075        tfxlm_roberta_model[0][1]        \n==================================================================================================\nTotal params: 559,893,507\nTrainable params: 559,893,507\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/5\n3004/3004 [==============================] - 922s 265ms/step - loss: 0.6127 - accuracy: 0.7104 - val_loss: 0.3499 - val_accuracy: 0.8740\nEpoch 2/5\n3004/3004 [==============================] - 780s 260ms/step - loss: 0.3204 - accuracy: 0.8805 - val_loss: 0.3337 - val_accuracy: 0.8855\nEpoch 3/5\n3004/3004 [==============================] - 781s 260ms/step - loss: 0.2436 - accuracy: 0.9109 - val_loss: 0.3476 - val_accuracy: 0.8851\nEpoch 00003: early stopping\nAverage Validation Loss: 0.343764861424764\nAverage Validation Accuracy: 0.8815330664316813\n#############################################\n\nFold 3 of 4\nTrain data shape: 384638, Validation data shape: 128212\nTrain indices: [     0      3      4 ... 512846 512848 512849], Validation indices: [     1      2     15 ... 512840 512841 512847]\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ntfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            3075        tfxlm_roberta_model[0][1]        \n==================================================================================================\nTotal params: 559,893,507\nTrainable params: 559,893,507\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/5\n3004/3004 [==============================] - 929s 267ms/step - loss: 0.7315 - accuracy: 0.6203 - val_loss: 0.3451 - val_accuracy: 0.8762\nEpoch 2/5\n3004/3004 [==============================] - 782s 260ms/step - loss: 0.3288 - accuracy: 0.8769 - val_loss: 0.3345 - val_accuracy: 0.8868\nEpoch 3/5\n3004/3004 [==============================] - 785s 261ms/step - loss: 0.2509 - accuracy: 0.9077 - val_loss: 0.3343 - val_accuracy: 0.8884\nEpoch 4/5\n3004/3004 [==============================] - 785s 261ms/step - loss: 0.1892 - accuracy: 0.9324 - val_loss: 0.3533 - val_accuracy: 0.8888\nEpoch 00004: early stopping\nAverage Validation Loss: 0.3417968526482582\nAverage Validation Accuracy: 0.8850536644458771\n#############################################\n\nFold 4 of 4\nTrain data shape: 384638, Validation data shape: 128212\nTrain indices: [     0      1      2 ... 512847 512848 512849], Validation indices: [     8     10     12 ... 512835 512839 512845]\n","output_type":"stream"},{"name":"stderr","text":"Some layers from the model checkpoint at jplu/tf-xlm-roberta-large were not used when initializing TFXLMRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFXLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_word_ids (InputLayer)     [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 120)]        0                                            \n__________________________________________________________________________________________________\ntfxlm_roberta_model (TFXLMRober TFBaseModelOutputWit 559890432   input_word_ids[0][0]             \n                                                                 input_mask[0][0]                 \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            3075        tfxlm_roberta_model[0][1]        \n==================================================================================================\nTotal params: 559,893,507\nTrainable params: 559,893,507\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/5\n3004/3004 [==============================] - 928s 266ms/step - loss: 0.6585 - accuracy: 0.6789 - val_loss: 0.3437 - val_accuracy: 0.8737\nEpoch 2/5\n3004/3004 [==============================] - 780s 260ms/step - loss: 0.3295 - accuracy: 0.8759 - val_loss: 0.3288 - val_accuracy: 0.8853\nEpoch 3/5\n3004/3004 [==============================] - 782s 260ms/step - loss: 0.2500 - accuracy: 0.9076 - val_loss: 0.3293 - val_accuracy: 0.8890\nEpoch 00003: early stopping\nAverage Validation Loss: 0.33390440543492633\nAverage Validation Accuracy: 0.8826838930447897\n#############################################\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='table'></a>\n\n| Fold | Avg Val Loss | Avg Val Accuracy |\n| --- | --- | --- |\n| 1 | 0.319 | 0.886 | \n| 2 | 0.343 | 0.882 | \n| 3 | 0.342 | 0.885 |\n| 4 | 0.333 | 0.883 |","metadata":{}},{"cell_type":"markdown","source":"We perform k-fold cross-validation on our best model and the above table documents how the model performs across each individual validation fold when k=4. We can see that the validation performance is stable and doesn't fluctuate much with ±0.024 in average validation loss and ±0.004 in validation accuracy across all folds. Hence we can say that our model is robust against small perturbations in the training data.","metadata":{}},{"cell_type":"code","source":"mean_val_loss = round(np.mean(val_loss_list), 3)\nmean_val_acc = round(np.mean(val_acc_list), 3)\n\nprint('Average validation loss over all folds: {}'.format(mean_val_loss))\nprint('Average validation accuracy over all folds: {}'.format(mean_val_acc))","metadata":{"papermill":{"duration":0.514332,"end_time":"2021-02-09T23:06:08.217489","exception":false,"start_time":"2021-02-09T23:06:07.703157","status":"completed"},"tags":[],"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Average validation loss over all folds: 0.335\nAverage validation accuracy over all folds: 0.884\n","output_type":"stream"}]}]}